{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val carRegistration = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./car_registration.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc);    \n",
    "\n",
    "val carRegistration = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./car_registration.csv\")\n",
    "\n",
    "carRegistration.take(1)\n",
    "\n",
    "case class info( speed: String, travel_time: String, date: String, link_id: String, link_points: String, encoded_poly_line: String, encoded_polyline_lvls: String, borough: String, link_name: String)\n",
    "\n",
    "val dateFormat1 = new java.text.SimpleDateFormat(\"yyyy MMM dd hh:mm:ss\")\n",
    "val dateFormat2 = new java.text.SimpleDateFormat(\"MM/dd/yyyy hh:mm:ss a\")\n",
    "val outputformat = new java.text.SimpleDateFormat(\"MM-dd-yyyy HH\");\n",
    "val contentRdd = carRegistration.filter( row => row.getAs[String](3).toInt >= 0).rdd\n",
    "                .map((row) => info(\n",
    "                                    row.getAs[String](1),//speed\n",
    "                                    row.getAs[String](2), //travel_time\n",
    "                                    if(row.getAs[String](4).size >= 23) \n",
    "                                    outputformat.format(dateFormat1.parse(row.getAs[String](4))) \n",
    "                                    else outputformat.format(dateFormat2.parse(row.getAs[String](4))), //date\n",
    "                                    row.getAs[String](5), //link_id\n",
    "                                    row.getAs[String](6), //link_points\n",
    "                                    row.getAs[String](7), //encoded_poly_line\n",
    "                                    row.getAs[String](8), //encoded_poly_line_lvls\n",
    "                                    row.getAs[String](11).toLowerCase, //borough\n",
    "                                    row.getAs[String](12) //link_name\n",
    "                                                       ))\n",
    "\n",
    "val pattern = \"\"\"(?<=\\()[^)]+(?=\\))\"\"\".r\n",
    "\n",
    "case class neighborhood(latitude: Double, longitude: Double, name: String, borough: String)\n",
    "\n",
    "def deg2rad(deg: Double):Double ={\n",
    "  deg * (Math.PI/180)\n",
    "}\n",
    "def getDistanceFromLatLonInKm(lat1: Double, lon1: Double, lat2: Double,lon2: Double):Double ={\n",
    "  val R = 6371; // Radius of the earth in km\n",
    "  val dLat = deg2rad(lat2-lat1);  // deg2rad below\n",
    "  val dLon = deg2rad(lon2-lon1); \n",
    "  val a = \n",
    "    Math.sin(dLat/2) * Math.sin(dLat/2) +\n",
    "    Math.cos(deg2rad(lat1)) * Math.cos(deg2rad(lat2)) * \n",
    "    Math.sin(dLon/2) * Math.sin(dLon/2)\n",
    "    ; \n",
    "  val c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a)); \n",
    "  val d = R * c; // Distance in km\n",
    "  R * c\n",
    "}\n",
    "\n",
    "def convertNeighborhood(row: org.apache.spark.sql.Row): neighborhood ={\n",
    "  \n",
    "  neighborhood(row.getAs[String](1).toDouble, row.getAs[String](2).toDouble, row.getAs[String](0).toLowerCase, row.getAs[String](3).toLowerCase)\n",
    "}\n",
    "\n",
    "val barrios = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./Neighborhoods.csv\")\n",
    "barrios.createOrReplaceTempView(\"export_csv\")\n",
    "\n",
    "\n",
    "val bronx = spark.sql(\"select Name, latitude, longitude, Borough from export_csv where Borough == \\\"Bronx\\\"\").rdd.map( convertNeighborhood(_) ).collect().toList\n",
    "val manhattan = spark.sql(\"select Name, latitude, longitude, Borough from export_csv where Borough == \\\"Manhattan\\\"\").rdd.map( convertNeighborhood(_) ).collect().toList\n",
    "val queens = spark.sql(\"select Name, latitude, longitude, Borough from export_csv where Borough == \\\"Queens\\\"\").rdd.map( convertNeighborhood(_) ).collect().toList\n",
    "val brooklyn = spark.sql(\"select Name, latitude, longitude, Borough from export_csv where Borough == \\\"Brooklyn\\\"\").rdd.map( convertNeighborhood(_) ).collect().toList\n",
    "val statenIsland = spark.sql(\"select Name, latitude, longitude, Borough from export_csv where Borough == \\\"Staten Island\\\"\").rdd.map( convertNeighborhood(_) ).collect().toList\n",
    "\n",
    "def menor(x:(Double, String) ,y:(Double, String)) = if(x._1 < y._1) x else y\n",
    "def checkPosition(latitude: Double, longitude: Double, borough: String): String = borough match{\n",
    "  case n if borough == \"bronx\" => bronx.map( x => (getDistanceFromLatLonInKm(latitude,longitude, x.latitude, x.longitude),x.name))\n",
    "                                                    .reduceLeft((x,y) => menor(x,y))._2\n",
    "  case n if borough == \"manhattan\" => manhattan.map( x => (getDistanceFromLatLonInKm(latitude, longitude, x.latitude, x.longitude),x.name))\n",
    "                                                              .reduceLeft((x,y) => menor(x,y))._2\n",
    "  case n if borough == \"queens\" => queens.map( x => (getDistanceFromLatLonInKm(latitude, longitude, x.latitude, x.longitude),x.name) )\n",
    "                                                        .reduceLeft((x,y) => menor(x,y))._2\n",
    "  case n if borough == \"brooklyn\" => brooklyn.map( x => (getDistanceFromLatLonInKm(latitude, longitude, x.latitude, x.longitude),x.name) )\n",
    "                                                            .reduceLeft((x,y) => menor(x,y))._2\n",
    "  case n if borough == \"staten island\" => statenIsland.map( x => (getDistanceFromLatLonInKm(latitude, longitude, x.latitude, x.longitude),x.name)).reduceLeft((x,y) => menor(x,y))._2\n",
    "}\n",
    "\n",
    "case class info2( speed: Double, travel_time: Double, date: String, latitud: String, longitud: String, \n",
    "                  encoded_poly_line: String, borough: String, link_name: String, neighborhood: String,\n",
    "                  humedad: String, presion: String, temperatura: String, descripcionClima: String, \n",
    "                  windDirection: String, windSpeed: String, accidentes: Int, heridos: Int, muertos: Int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import java.util.Calendar\n",
    "val dateFormatExpand = new java.text.SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\")\n",
    "\n",
    "case class data( manhattan: String, queens: String, brooklyn: String, statenIsland: String, bronx: String)\n",
    "val cal = Calendar.getInstance()\n",
    "def dates(x: org.apache.spark.sql.Row): List[(String,data)]={\n",
    "  cal.setTime(dateFormatExpand.parse(x.getAs[String](0)))\n",
    "  cal.add(Calendar.DAY_OF_MONTH, 29)\n",
    "  cal.add(Calendar.YEAR, 5) \n",
    "  cal.add(Calendar.MONTH, 1)\n",
    "  List( x.getAs[String](0) -> data( x.getAs[String](1), x.getAs[String](2), x.getAs[String](3), x.getAs[String](4), x.getAs[String](5)),\n",
    "  outputformat.format(cal.getTime) -> data(x.getAs[String](1), x.getAs[String](2), x.getAs[String](3), x.getAs[String](4), x.getAs[String](5)))\n",
    "}\n",
    "\n",
    "val humedad = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./humedad/humedad.csv\")\n",
    "val eHumedad = humedad.rdd.map(dates).flatMap(x => x).collect.toMap.withDefaultValue(data(null,null,null,null,null))\n",
    "\n",
    "val pressure = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./pressure/pressure.csv\")\n",
    "val ePressure = pressure.rdd.map(dates).flatMap(x => x).collect.toMap.withDefaultValue(data(null,null,null,null,null))\n",
    "\n",
    "val temperature = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./temperature/temperature.csv\")\n",
    "val eTemperature = temperature.rdd.map(dates).flatMap(x => x).collect.toMap.withDefaultValue(data(null,null,null,null,null))\n",
    "\n",
    "val weatherDescription = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./weatherDescription/weatherDescription.csv\")\n",
    "val eWeatherDescription = weatherDescription.rdd.map(dates).flatMap(x => x).collect.toMap.withDefaultValue(data(null,null,null,null,null))\n",
    "\n",
    "val windDirection = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./windDirection/windDirection.csv\")\n",
    "val eWindDirection = windDirection.rdd.map(dates).flatMap(x => x).collect.toMap.withDefaultValue(data(null,null,null,null,null))\n",
    "\n",
    "val windSpeed = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./windSpeed/windSpeed.csv\")\n",
    "val eWindSpeed = windSpeed.rdd.map(dates).flatMap(x => x).collect.toMap.withDefaultValue(data(null,null,null,null,null))\n",
    "\n",
    "\n",
    "case class weather(humedad: String, presion: String, temperatura: String, descripcionClima: String, \n",
    "                   windDirection: String, windSpeed: String)\n",
    "\n",
    "def addWeather(borough: String, fecha: String): weather=\n",
    "  borough match{\n",
    "    case \"manhattan\" => weather(eHumedad(fecha).manhattan, ePressure(fecha).manhattan,\n",
    "                                    eTemperature(fecha).manhattan, eWeatherDescription(fecha).manhattan,\n",
    "                                    eWindDirection(fecha).manhattan, eWindSpeed(fecha).manhattan)\n",
    "    \n",
    "    case \"brooklyn\" => weather(eHumedad(fecha).brooklyn, ePressure(fecha).brooklyn,\n",
    "                                    eTemperature(fecha).brooklyn, eWeatherDescription(fecha).brooklyn,\n",
    "                                    eWindDirection(fecha).brooklyn, eWindSpeed(fecha).brooklyn)\n",
    "    \n",
    "    case \"queens\" => weather(eHumedad(fecha).queens, ePressure(fecha).queens,\n",
    "                                    eTemperature(fecha).queens, eWeatherDescription(fecha).queens,\n",
    "                                    eWindDirection(fecha).queens, eWindSpeed(fecha).queens)\n",
    "    \n",
    "    case \"bronx\" => weather(eHumedad(fecha).bronx, ePressure(fecha).bronx,\n",
    "                                    eTemperature(fecha).bronx, eWeatherDescription(fecha).bronx,\n",
    "                                    eWindDirection(fecha).bronx, eWindSpeed(fecha).bronx)\n",
    "    \n",
    "    case \"staten island\" => weather(eHumedad(fecha).statenIsland, ePressure(fecha).statenIsland,\n",
    "                                    eTemperature(fecha).statenIsland, eWeatherDescription(fecha).statenIsland,\n",
    "                                    eWindDirection(fecha).statenIsland, eWindSpeed(fecha).statenIsland)\n",
    "  }\n",
    "\n",
    "  \n",
    "\n",
    "val dateFormatAccidentes = new java.text.SimpleDateFormat(\"MM/dd/yyyy HH:mm\")\n",
    "\n",
    "\n",
    "val accidentes = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./accidentes.csv\")\n",
    "case class accidentData (fecha: String, latitud: String, longitud: String, heridos: Int, muertos: Int, \n",
    "                         borough: String, street: String)\n",
    "\n",
    "def parseable(fecha: String): String ={\n",
    "  var str: String = null\n",
    "  try{\n",
    "      str = outputformat.format(dateFormatAccidentes.parse(fecha))\n",
    "  }catch{\n",
    "    case x: Exception => {return null}\n",
    "    \n",
    "  }\n",
    "  return str; \n",
    "} \n",
    "\n",
    "val cleaned = accidentes.rdd.filter(x => parseable(x.getAs[String](0)+\" \"+x.getAs[String](1)) != null && \n",
    "                      x.getAs[String](2) != null &&\n",
    "                      x.getAs[String](4) != null && \n",
    "                      x.getAs[String](5) != null &&\n",
    "                      x.getAs[String](7) != null)\n",
    "\n",
    "def sp(num: String): Int ={\n",
    "  try{\n",
    "    val number = num.toInt\n",
    "    return number\n",
    "  }catch{\n",
    "    case x: Exception => {return 0}\n",
    "  }\n",
    "} \n",
    "\n",
    "def convert(x: org.apache.spark.sql.Row): accidentData={\n",
    "  val fecha: String = parseable(x.getAs[String](0)+\" \"+x.getAs[String](1))\n",
    "  val heridos: Int = sp(x.getAs[String](10))+sp(x.getAs[String](12))+sp(x.getAs[String](14))+sp(x.getAs[String](16))\n",
    "  val muertos: Int = sp(x.getAs[String](11))+sp(x.getAs[String](13))+sp(x.getAs[String](15))+sp(x.getAs[String](17))\n",
    "  accidentData(fecha, x.getAs[String](4),x.getAs[String](5), heridos, muertos, \n",
    "               x.getAs[String](2).trim.toLowerCase, x.getAs[String](7).trim.toLowerCase)\n",
    "}\n",
    "\n",
    "val accident_data = cleaned.map(convert)\n",
    "case class accidentData2 (fecha: String, latitud: String, longitud: String, heridos: Int, muertos: Int, \n",
    "                          nAccidentes: Int, id: Long)\n",
    "var i: Long = 0;\n",
    "def grouped(x: ((String, String),Iterable[accidentData])): accidentData2 ={\n",
    "    val nAccidentes = x._2.size\n",
    "    val heridos = x._2.foldLeft(0)( (x,y) => x + y.heridos )\n",
    "    val muertos = x._2.foldLeft(0)( (x,y) => x + y.muertos )\n",
    "    val item = x._2.last\n",
    "    i = i + 1\n",
    "    accidentData2(x._1._1 ,item.latitud, item.longitud, heridos, muertos, nAccidentes, i)\n",
    "}\n",
    "val inmutableSet = accident_data.groupBy(x => (x.fecha,x.street)).map(grouped).collect.toSet\n",
    "\n",
    "\n",
    "def procesAgruped(x:((Int, String), Iterable[info]) ): info2 ={\n",
    "    val item = x._2.last\n",
    "    val size = x._2.size\n",
    "    val v_prom = x._2.foldLeft(0D)( (x,y) => x+y.speed.toDouble) / size\n",
    "    val t_prom = x._2.foldLeft(0D)( (x,y) => x+y.travel_time.toDouble) / size\n",
    "    val points = item.link_points.split(\" \").map(_.split(\",\"))\n",
    "    val lat =  points(0)(0).toDouble\n",
    "    val lon = points(0)(1).toDouble\n",
    "    val check = mutableAccidents.filter(y => y.fecha == x._1._2)\n",
    "    var accidentes = 0\n",
    "    var muertos = 0\n",
    "    var heridos = 0\n",
    "    if(!check.isEmpty){\n",
    "        val accident = check.map(y => (getDistanceFromLatLonInKm(y.latitud.toDouble,y.longitud.toDouble,\n",
    "                                                             lat.toDouble,lon.toDouble), y))\n",
    "                                  .reduceLeft((x,y) => if(x._1 < y._1) x else y)._2\n",
    "        mutableAccidents -= accident\n",
    "        accidentes = accident.nAccidentes\n",
    "        muertos = accident.muertos\n",
    "        heridos = accident.heridos\n",
    "    }\n",
    "    \n",
    "    val w = addWeather(item.borough, x._1._2)\n",
    "    info2(v_prom, t_prom, x._1._2, lat.toString, lon.toString, item.encoded_poly_line, \n",
    "          item.borough, item.link_name, checkPosition(lat, lon, item.borough), w.humedad, w.presion, \n",
    "          w.temperatura, w.descripcionClima, w.windDirection, w.windSpeed, accidentes, heridos, muertos)\n",
    "}\n",
    "\n",
    "val dataAgruped = contentRdd.groupBy(x => (x.link_id.toInt, x.date))\n",
    "\n",
    "var a,b,c,d,e,f = \"\"\n",
    "def checkAndReplace2(x: info2): info2 ={\n",
    "  if(x.presion == null){\n",
    "  return info2(\n",
    "    x.speed,\n",
    "    x.travel_time,\n",
    "    x.date,\n",
    "    x.latitud,\n",
    "    x.longitud,\n",
    "    x.encoded_poly_line,\n",
    "    x.borough,\n",
    "    x.link_name,\n",
    "    x.neighborhood,\n",
    "    a,\n",
    "    b,\n",
    "    c,\n",
    "    d,\n",
    "    e,\n",
    "    f,\n",
    "    x.accidentes,\n",
    "    x.heridos,\n",
    "    x.muertos)\n",
    "  }else{\n",
    "    a = x.humedad\n",
    "    b = x.presion\n",
    "    c = x.temperatura\n",
    "    d = x.descripcionClima\n",
    "    e = x.windDirection\n",
    "    f = x.windSpeed\n",
    "    return x\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "val procesed = dataAgruped.map(procesAgruped(_)).map(checkAndReplace2).filter(x => !x.humedad.isEmpty || x.humedad != null)\n",
    "\n",
    "procesed.toDF.repartition(1).write.option(\"header\", \"true\").csv(\"./datasetFinalIntegrador2\") \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val procesedDataset = sqlContext.read.format(\"csv\").option(\"header\", \"true\").load(\"./procesedAccidents/procesedAccidents.csv\")\n",
    "\n",
    "case class info2( speed: String, travel_time: String, date: String, latitud: String, longitud: String, \n",
    "                  encoded_poly_line: String, borough: String, link_name: String, neighborhood: String,\n",
    "                  humedad: String, presion: String, temperatura: String, descripcionClima: String, \n",
    "                  windDirection: String, windSpeed: String, accidentes: Int, heridos: Int, muertos: Int)\n",
    "\n",
    "import scala.util.Random\n",
    "def realAgrupedAccidents(x: ((String,String), Iterable[org.apache.spark.sql.Row])):List[info2] = {\n",
    "    val shuffled = Random.shuffle(x._2)\n",
    "    val item = shuffled.last\n",
    "    val struct = shuffled.init\n",
    "    val lat = item.getAs[String](3)\n",
    "    val lon = item.getAs[String](4)\n",
    "    val check = inmutableSet.filter(y => y.fecha == item.getAs[String](2))\n",
    "    var accidentes = 0\n",
    "    var muertos = 0\n",
    "    var heridos = 0\n",
    "    if(!check.isEmpty){\n",
    "        val accident = check.map(y => (getDistanceFromLatLonInKm(y.latitud.toDouble,y.longitud.toDouble,\n",
    "                                                             lat.toDouble,lon.toDouble), y))\n",
    "                                  .reduceLeft((x,y) => if(x._1 < y._1) x else y)._2\n",
    "       \n",
    "        accidentes = accident.nAccidentes\n",
    "        muertos = accident.muertos\n",
    "        heridos = accident.heridos\n",
    "    }\n",
    "    val acc = info2(  \n",
    "            item.getAs[String](0),\n",
    "            item.getAs[String](1),\n",
    "            item.getAs[String](2),\n",
    "            item.getAs[String](3),\n",
    "            item.getAs[String](4),\n",
    "            item.getAs[String](5),\n",
    "            item.getAs[String](6),\n",
    "            item.getAs[String](7),\n",
    "            item.getAs[String](8),\n",
    "            item.getAs[String](9),\n",
    "            item.getAs[String](10),\n",
    "            item.getAs[String](11),\n",
    "            item.getAs[String](12),\n",
    "            item.getAs[String](13),\n",
    "            item.getAs[String](14),\n",
    "            accidentes,\n",
    "            heridos,\n",
    "            muertos\n",
    "            )\n",
    "    val finalStruct = acc :: (struct.map( y => info2(\n",
    "        y.getAs[String](0),\n",
    "        y.getAs[String](1),\n",
    "        y.getAs[String](2),\n",
    "        y.getAs[String](3),\n",
    "        y.getAs[String](4),\n",
    "        y.getAs[String](5),\n",
    "        y.getAs[String](6),\n",
    "        y.getAs[String](7),\n",
    "        y.getAs[String](8),\n",
    "        y.getAs[String](9),\n",
    "        y.getAs[String](10),\n",
    "        y.getAs[String](11),\n",
    "        y.getAs[String](12),\n",
    "        y.getAs[String](13),\n",
    "        y.getAs[String](14),\n",
    "        0,\n",
    "        0,\n",
    "        0)).toList)\n",
    "    \n",
    "    finalStruct.toList\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
